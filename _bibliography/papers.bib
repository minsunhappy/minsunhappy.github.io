---
---

@article{kim2024GenHi,
  title={Generating Highlight Videos of a User-specified Length using Most Replayed Data},
  author={Kim, Minsun and Lee‚Ä†, Dawon and Noh‚Ä†, Junyong},
  journal={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  abstract={A highlight is a short edit of the original video that includes the most engaging moments. Given the rigid timing of TV commercial slots and length limits of social media uploads, generating highlights of specific lengths is crucial. Previous research on automatic highlight generation often overlooked the control over the duration of the final video, producing highlights of arbitrary lengths. We propose a novel system that automatically generates highlights of any user-specified length. Our system leverages Most Replayed Data (MRD), which identifies how frequently a video has been watched over time, to gauge the most engaging parts. It then optimizes the final editing path by adjusting internal segment durations. We evaluated the quality of our system's outputs through two user studies, including a comparison with highlights created by human editors. Results show that our system can automatically produce highlights that are indistinguishable from those created by humans in viewing experience.},
  year={2025},
  annotation={‚Ä† These authors contributed equally as corresponding authors.},
  abbr={HCI},
  selected={true},
  preview = {highlight.png}
}

@article{kim2024video,
  title={üèÜ Video classifier with adaptive blur network to determine horizontally extrapolatable video content},
  author={Kim, Minsun and Seo, Changwook and Yun, Hyun Ho and Noh, Junyong},
  abstract={While the demand for extrapolating video content horizontally or vertically is increasing, even the most advanced techniques cannot successfully extrapolate all videos. Therefore, it is important to determine if a given video can be well extrapolated before attempting the actual extrapolation. This can help avoid wasting computing resources. This paper proposes a video classifier that can identify if a video is suitable for horizontal extrapolation. The classifier utilizes optical flow and an adaptive Gaussian blur network, which can be applied to flow-based video extrapolation methods. The labeling for training was rigorously conducted through user tests and quantitative evaluations. As a result of learning from this labeled dataset, a network was developed to determine the extrapolation capability of a given video. The proposed classifier achieved much more accurate classification performance than methods that simply use the original video or fixed blur alone by effectively capturing the characteristics of the video through optical flow and adaptive Gaussian blur network. This classifier can be utilized in various fields in conjunction with automatic video extrapolation techniques for immersive viewing experiences.},
  journal={Journal of the Korea Computer Graphics Society},
  award ={üèÜ Best Paper Award},
  volume={30},
  number={3},
  pages={99--107},
  year={2024},
  publisher={Korea Computer Graphics Society},
  abbr={Computer Vision},
  url = {https://koreascience.kr/article/JAKO202424941304832.page},
  doi = {https://doi.org/10.15701/kcgs.2024.30.3.99},
  preview = {video_extra.jpg},
}


